import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import numpy as np


# RSS 뉴스 URL
news_rss_url = {
    '매일경제' : 'https://www.mk.co.kr/rss/50200011/',
    '한국경제' : 'https://www.hankyung.com/feed/finance' 
}


# 기사 1건의 정보를 저장하기위한 자료구조
class Article :
    def __init__(self, *, media, datetime, title, content):
        self.media = media         # 언론사명
        self.datetime = datetime   # 작성일시
        self.title = title         # 뉴스제목
        self.content = content     # 뉴스내용


# 뉴스기사 수집하기
def get_news_rss(media, url) :   
    """doc-string
    매개변수: media-언론사명, url-언론사의 rss url
    반환값 : 기사를 요소로 갖는 목록
    """
    article_list = [] # 기사 목록 (Ariticle객체를 요소로 갖는 리스트)
    
    title_list = []  # 기사 제목 목록
    link_list = []   # 기사 링크 목록
    news_data = []   # 기사 본문 목록
    news_datetime = [] # 기사 작성일시 목록

    # 수집봇이 아닌 브라우져가 접근한것 처럼 요청하기위함
    headers = {
        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
    }    
    news_rss = requests.get(url,headers)
    if news_rss.status_code != 200 :
        raise Exception('응답오류 발생')
        
    news_rss_soup = BeautifulSoup(news_rss.content,'xml')
    # 기사 제목 
    title_list = news_rss_soup.select('item > title')
    title_list = [title.text for title in title_list]
    # 기사 상세링크 
    link_list = news_rss_soup.select('item > link')
    link_list = [link.text for link in link_list]
    # 기사 내용, 기사 작성일시
    for link in link_list :
        news_res = requests.get(link, headers=headers)
        news_content_soup = BeautifulSoup(news_res.content, 'lxml')

        try:
            if media == '매일경제':
                news_content = news_content_soup.select_one('.news_cnt_detail_wrap')
                news_data.append(news_content.text)
                news_datetime.append(news_content_soup.select_one('.time_area .registration dd').text)    
            elif media == '한국경제':
                news_content = news_content_soup.select_one('#articletxt')
                news_data.append(news_content.text)
                news_datetime.append(news_content_soup.select_one('.datetime > .item > .txt-date').text)    
        except Exception as e:
            print(len(news_data),link)
            print(e)
            
    # print(title_list)
    # print(news_data)
    # print(news_datetime)
    print(len(title_list), len(news_data), len(news_datetime))
    for item in zip(title_list, news_data, news_datetime):
        # print(media, item[0],item[1],item[2])
        article = Article(media = media, title=item[0], content=item[1], datetime=item[2])
        article_list.append(article)    
    
    return article_list


# 모든 언론사 기사를 합치기
all_article_list = []
for media, url in news_rss_url.items():
    print(media,url)
    all_article_list.append(get_news_rss(media,url))
print(len(all_article_list))


result = np.array(all_article_list).flatten().tolist()
print(len(result))
print(type(result))


data = {
    '언론사' : [ article.media for article in result ],
    '제목' : [ article.title for article in result ],
    '내용' : [ article.content for article in result ],
    '작성일시' : [ article.datetime for article in result ],
}


df = pd.DataFrame(data)


df.head(3)


# 날짜는 파이썬 표준라이브러리 datetime 사용
import datetime
# 오늘 날짜 구하기
today = datetime.datetime.today()
today = today.strftime('%Y-%m-%d')
today


df.to_csv(f'news_{today}.csv',index=False)






