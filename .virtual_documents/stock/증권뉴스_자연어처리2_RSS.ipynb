# !pip install konlpy
# !pip install koreanize_matplotlib


# 데이터 분석 패키지
import pandas as pd
import numpy as np


# 모든 컬럼을 출력하도록 설정
pd.set_option('display.max_columns', None)  # None으로 설정하면 모든 컬럼 출력 default:20
# 모든 행을 출력하도록 설정
pd.set_option('display.max_rows', 10)  # None으로 설정하면 모든 행 출력 default:50
# 모든 열의 최대 너비를 설정
pd.set_option('display.max_colwidth', 60) # None으로 설정하면 모든 내용을 출력 default:60


pd.set_option?


# 시각화 패키지
import matplotlib.pyplot as plt
import seaborn as sns
import koreanize_matplotlib
from wordcloud import WordCloud


# 정규표현식
import re


# 날짜는 파이썬 표준라이브러리 datetime 사용
import datetime


# 오늘 날짜 구하기
today = datetime.datetime.today()
print(today)
today = today.strftime('%Y-%m-%d')
today


df = pd.read_csv(f'news_preprocessed_{today}.csv')
df.head(1)





corpus = df['제목'] #.head(10)
corpus


from sklearn.feature_extraction.text import CountVectorizer


CountVectorizer?





cvect = CountVectorizer(max_df=5,min_df=2,ngram_range=(1,2))





X = cvect.fit_transform(corpus)
X


# 단어사전 확인
# 단어사전은 {"단어":인덱스번호}
cvect.vocabulary_


X.toarray()


cvect.get_feature_names_out()


cvect.get_feature_names_out().size


tmp = pd.DataFrame(X.toarray(), columns=cvect.get_feature_names_out())


tmp.head(2)


cvect_voca = tmp.sum().sort_values(ascending=False)


# 모델을 받아서 변환후 문서-어휘 행렬로 반환하는 함수
def display_transform_dtm(cvect, corpus):
    '''
    모델을 받아 변환을 하고 문서-어휘 행렬을 반환하는 함수
    '''
    X = cvect.fit_transform(corpus)
    print(cvect.get_feature_names_out()) # 어휘사전 출력
    dtm = X.toarray()
    df_dtm = pd.DataFrame(dtm,columns=cvect.get_feature_names_out()).style.background_gradient()
    return df_dtm


display_transform_dtm(cvect,corpus)





from sklearn.feature_extraction.text import TfidfVectorizer


TfidfVectorizer?


# cvect = CountVectorizer(max_df=5,min_df=2,ngram_range=(1,2))


tfidfvect = TfidfVectorizer(max_df=5,min_df=2,ngram_range=(1,2))
X = tfidfvect.fit(corpus).transform(corpus)  # tfidfvect.fit_transform(corpus) 와 동일
X


# 문서에 토큰이 더 많이 나타날수록 가중치는 더 커진다 TF
# 그러나 토큰이 여러문서에 많이 표시될수록 가중치는 감소한다. DF
dtm = X.toarray()
dtm


display_transform_dtm(tfidfvect, corpus)


tmp = pd.DataFrame(X.toarray(), columns=cvect.get_feature_names_out())


tfidvect_voca = tmp.sum().sort_values(ascending=False)


print(tfidfvect.idf_.shape)
print(tfidfvect.idf_)


# idf_dict 값 시각화
pd.Series(idf_dict).nsmallest(30).to_frame().style.background_gradient()


# idf_dict 값 시각화
pd.Series(idf_dict).nlargest(30).to_frame().style.background_gradient()


pd.Series(idf_dict).nsmallest(30).to_frame().plot.barh()


def display_word_cloud(dict, max_words=30, width=1200, height=600) :
    font_path = r'C:\Windows\Fonts\malgun.ttf'
    stopwords = ['코스피','코스닥','종목','ETF','주식','주가','상승','하락','상장','투자','서학','동학','기업','시장'
                 '시총','목표가','올해','내년','국내','해외','외국인','소식에','기대감','한국','증시']
    word_cloud = WordCloud(font_path=font_path, 
                      width=width, 
                      height=height,
                      stopwords=stopwords,
                      background_color='white',
                      max_words = max_words,
                      min_word_length = 2,
                      random_state=2024
                    ).generate_from_frequencies(dict)
    plt.imshow(word_cloud)
    plt.show()
    return word_cloud


# CounterVectorizer로 분석한 어휘빈도 내림차순
dict = cvect_voca.to_dict()
print(dict)
display_word_cloud(dict,100)


# TfidfVectorizer로 분석한 어휘중요도 내림차순
dict = tfidvect_voca.to_dict()
print(dict)
display_word_cloud(dict,100)



